\documentclass[svgnames,final]{beamer}
\usepackage{etex}
\mode<presentation> {
  \usetheme{I6dv}
}
\usepackage[utf8]{inputenc}
%\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage{amsmath,amsfonts,amsthm}
\usepackage{mathtools}
\usepackage{booktabs}
\usepackage[orientation=landscape,size=a0,scale=1.4]{beamerposter}
\usepackage{pgfplots}
\usepackage{comment}
% \usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{etoolbox}
\pgfplotsset{compat=1.12}
\usepackage{tikz}
\usetikzlibrary{positioning,plotmarks,calc,arrows,decorations.markings,intersections,backgrounds}
\usepackage{amsmath,amsfonts,amsthm,bm} % Math packages


\newcommand{\thetatilde}{\ensuremath{\tilde{\theta}}}
%\tikzexternalize[prefix=tikz_output/]

\usepackage [english]{babel}
\usepackage [autostyle, english = american]{csquotes}
\MakeOuterQuote{"}

\newtoggle{plotexperiments}
\toggletrue{plotexperiments}
%\togglefalse{plotexperiments}

\newcommand{\numplotsamples}{200}

\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\green}[1]{\textcolor{Green}{#1}}
\newcommand{\blue}[1]{\textcolor{blue}{#1}}
\newcommand{\eps}{\ensuremath{\epsilon}}
\newcommand{\R}{\ensuremath{\mathbb{R}}}
\newcommand{\setC}{\ensuremath{\mathcal{C}}}
\newcommand{\thetahat}{\ensuremath{\hat{\theta}}}
\newcommand{\thetastar}{\ensuremath{\theta^*}}
\newcommand{\Otilde}{\ensuremath{\widetilde{O}}}
\newcommand{\OPT}{\ensuremath{\textnormal{OPT}}}
\newcommand{\projH}{\ensuremath{\mathcal{H}}}
\newcommand{\projT}{\ensuremath{\mathcal{T}}}
\newcommand{\Usubspaces}{\ensuremath{\mathbb{U}}}
\DeclarePairedDelimiter{\abs}{\lvert}{\rvert}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}
\newcommand{\specialcell}[2][c]{%
  \begin{tabular}[#1]{@{}c@{}}#2\end{tabular}}

\newcommand{\prospoint}[1]{\textcolor{Green}{Pros: #1}}
\newcommand{\conspoint}[1]{\textcolor{red}{Cons: #1}}

\newcommand{\greencell}[1]{\green{#1}}
\newcommand{\redcell}[1]{\red{#1}}
\newcommand{\yellowcell}[1]{\textcolor{orange!40!yellow}{#1}}

\title{Graph-Sparse Logistic Regression}
\author{Alexander LeNail$^1$, Ludwig Schmidt$^2$, Jonathan Li$^1$, Tobias Ehrenberger$^1$, Karen Sachs$^1$, Stefanie Jegelka$^2$, Ernest Fraenkel$^1$}
\institute{$^1$MIT BE, $^2$MIT CSAIL}

\begin{document}

\newlength{\gextlength}
\setlength{\gextlength}{14pt}
\newlength{\gselradius}
\setlength{\gselradius}{38pt}

\begin{frame}
\vspace{-.5cm}
\begin{columns}[T]

\begin{column}{.3\linewidth}

	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\begin{block}{Problem setup}
		
    \vspace{-.8cm}

    \textbf{Variable selection} in a linear model:
    {\Large
    \[
      y \; = \; \sigma(X \,  \thetastar)
    \]
    }

    \vspace{-.7cm}

    \begin{itemize}
      \item Data matrix $X \in \R^{n \times d}$\\[.2cm]
      \item Unknown parameters $\thetastar \in \R^d$\\[.2cm]
      \item Binary labels $y \in \{0,1\}^n$\\[.2cm]
      \item $\sigma: \R \rightarrow \R$ is the logistic function $\sigma(x) = \frac{1}{1 + e^{-x}}$
    \end{itemize}

    \vspace{1.2cm}
    \textbf{This work:} our goal is to select a \textbf{graph-sparse} set of variables.\\[.5cm]
    $\quad\rightarrow$ \textbf{Statistical efficiency:} fewer samples for same error.\\[.3cm]
    $\quad\rightarrow$ \textbf{Interpretability:} graph-structure in many applications.

    \vspace{1.2cm}
    \textbf{Graph sparsity}
		\begin{itemize}
  		\item Every variable (parameter index) corresponds to a node.\\[.2cm]
			\item Selected variables form a \textbf{connected subgraph.}
    \end{itemize}

    \input{graph_sparsity_picture}

	\end{block}

	\vspace{1.0cm}

	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\begin{block}{Our motivation: graph-structed data in biology}
		Protein expression data with a \textbf{protein-protein interation network.}\\[-.2cm]
		\begin{figure}[h]
		\centering
		\includegraphics[width=.8\linewidth]{images/matrix.pdf}
		\end{figure}

		Graph given by prior knowledge from biology:\\[-.6cm]

		\begin{figure}[h]
		\centering
		\includegraphics[width=0.7\linewidth]{images/graph.pdf}
		\end{figure}

		\vspace{-1.5cm}

	\end{block}

	\vspace{2cm}

\end{column}

\begin{column}{.3\linewidth}

	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\begin{block}{Our approach}
    We build on results in \textbf{compressive sensing} for graph-sparse data.\\[.3cm]
		$\qquad$\textcolor{darkgray}{[Huang, Zhang, Metaxas, 2011], [Hegde, Indyk, Schmidt, 2015]}
		\vspace{1.2cm}

		$\rightarrow$ We introduce \textbf{Graph-Sparse Logistic Regression} (GSLR).\\[.2cm]
		\begin{itemize}
			\item Gradient descent on logistic loss.\\[.2cm]
			\item Efficient projections onto the graph sparse set.
		\end{itemize}

		\vspace{1cm}
    
		\begin{tikzpicture}
    \node [rectangle,draw,black,line width=.2cm,text width=33cm,align=left] {\begin{algorithmic}[1]
			\vspace{-.3cm}
			\Function{GSLR}{$X$, $y$, $G$, $s$, $\eta$, $k$}
			\State Let $f(X, y, \theta)$ be the logistic loss.
			\State $\thetahat^0 \gets 0$
			\For{$i \gets 0, \ldots, k-1$}
			\State $\thetatilde^{i+1} \gets \thetahat^i - \eta \cdot \nabla f(X, y, \thetahat^i)$
			\State $\thetahat^{i+1} \gets P_{G,s}(\thetatilde^{i+1})$ \Comment{Graph-sparse projection \hspace{.5cm}$ \;$ }
			\EndFor
			\State \textbf{return} $\thetahat^k$
			\EndFunction
			\vspace{.7cm}
			\end{algorithmic}};
    \end{tikzpicture}
	\end{block}

	\vspace{1.0cm}

	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\begin{block}{Efficient graph-sparse projections}
	\vspace{-.5cm}
	\textbf{Projection problem}: Given $b \in \R^d$ and a graph-sparse set $\mathbb{G}$, find
	{\Large
	\[
		\Omega^* \; = \; \argmin_{\Omega \in \mathbb{G}} \, \norm{b  - b_{\Omega}} \; .
	\]
	}
	\input{middle-col-low.tex}
	\end{block}
\end{column}

\begin{column}{.3\linewidth}

	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\begin{block}{Experimental Setup}

		Since we don't have the ground truth subgraphs for the real Ovarian Cancer data,
		we generate synthetic data by this procedure:

		1. Determine $\mu$ and $\bm{\Sigma}$ from real Ovarian Cancer Proteomics data.

		2. Sample from multivariate $\mathcal{N} (\vec{\mu}, \bm{\Sigma} )$

		3. Sample perturbation vector $\vec{x}$:

			\setlength\parindent{96pt}

			scheme 1: $\vec{x_p} = \mathcal{N} (0, \sigma_p^2 )$ if $p \in$ KEGG, 0 otherwise

			scheme 2: $\vec{x_p} = \mathcal{N} (\pm\sigma_p, \sigma_p^2 )$ if $p \in$ KEGG, 0 otherwise

			\setlength\parindent{0pt}

		4. Translate "positive" samples by perturbation vector


		\begin{figure}[h]
		  \centering
		\begin{tikzpicture}[
		declare function={mu1=1;},
		declare function={mu2=2;},
		declare function={mu3=2;},
		declare function={sigma1=0.5;},
		declare function={sigma2=1;},
		declare function={normal(\m,\s)=1/(2*\s*sqrt(pi))*exp(-(x-\m)^2/(2*\s^2));},
		declare function={bivar(\ma,\sa,\mb,\sb)=3.3/(2*pi*\sa*\sb) * exp(-((x-\ma)^2/\sa^2 + (y-\mb)^2/\sb^2))/2;}]
		\begin{axis}[
		ticks=none,
		colormap name=whitered,
		width=18cm,
		view={35}{65},
		enlargelimits=false,
		grid=major,
		domain=-1:4,
		y domain=-1:4,
		samples=26,
		xlabel=$x_1$,
		ylabel=$x_2$,
		zlabel={$P$},
		]

		%\addplot3 [opacity=1,surf,colormap={whitered}{color(0cm)=(white); color(1cm)=(orange!75!red)}] {bivar(mu1,sigma1,mu2,sigma2)};
		%\addplot3 [opacity=0.5,surf,colormap={whiteyellow}{color(0cm)=(white); color(1cm)=(orange!75!yellow)}] {bivar(mu3,sigma1,mu2,sigma2)};

		%\addplot3 [domain=-1:4,samples=31, samples y=0, thin, smooth] (x,4,{normal(mu1,sigma1)});
		%\addplot3 [domain=-1:4,samples=31, samples y=0, thin, smooth] (-1,x,{normal(mu2,sigma2)});
		\addplot3 [domain=-1:4,samples=31, samples y=0, thin, smooth] (x,4,{normal(mu3,sigma1)});

		\draw [black!50] (axis cs:-1,0,0) -- (axis cs:4,0,0);
		\draw [black!50] (axis cs:0,-1,0) -- (axis cs:0,4,0);

		\end{axis}
		\end{tikzpicture}
		\end{figure}


		Since we know the perturbation vector, we know the ground truth!

		We can then evaluate algorithms on the feature selection task.

	\end{block}


	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\begin{block}{Experimental Results}

		We benchmark our technique against the LASSO by how many of the truly "perturbed" features each uses in its support.

		\begin{figure}[h]
		\centering
		%\begin{subfigure}{.5\textwidth}
		%  \centering
		  \includegraphics[width=.5\linewidth]{images/2.pdf}
		%\end{subfigure}%
		%\begin{subfigure}{.5\textwidth}
		%  \centering
		  \includegraphics[width=.5\linewidth]{images/1.pdf}
    \newline
		%\end{subfigure}
		%\begin{subfigure}{.5\textwidth}
		%  \centering
		  \includegraphics[width=.5\linewidth]{images/4.pdf}
		%\end{subfigure}%
		%\begin{subfigure}{.5\textwidth}
		%  \centering
		  \includegraphics[width=.5\linewidth]{images/3.pdf}
		%\end{subfigure}
		\label{fig:perf}
		\end{figure}

		We then use our technique on real Ovarian Cancer data,
		and find that the support chosen by GSLR is qualitatively superior.

	\end{block}

	\vspace{2cm}

	% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	% \begin{block}{Conclusion}



	% \end{block}

\end{column}
\end{columns}
\end{frame}
\end{document}
